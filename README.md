# CS 247: Advanced Data Mining
### Instructor: Yizhou Sun
- Lecture Time: Monday/Wednesday 2-3:50pm 
- Classroom: https://ucla.zoom.us/j/94865976682?pwd=MTZqM054aG1NeXVhVlFmbnV1WExNQT09 for the first two weeks
- Office hours: Monday/Tuesday 4-5pm @ zoom above

### TA:
- Zongyue Qin (qinzongyue at cs.ucla.edu), office hours: Tuesday 12:00-13:00 Thursday 18:00-19:00 (Tentative), Zoom (Zoom link is posted on canvas/bruinlearn)
- Discussion Time: Friday 2-3:50pm
- Classroom: Kinsey Science Teaching Pavilion 1220B

## Course Description
This course introduces concepts, algorithms, and techniques of data mining on different types of datasets, which covers basic data mining algorithms, as well as advanced topics on text mining, graph/network mining, and recommender systems. A team-based course project involving hands-on practice of mining useful knowledge from large data sets is required, in addition to regular assignments. The course is a graduate-level computer science course, which is also a good option for senior undergraduate students who are interested in the field as well as students from other disciplines who need to understand, develop, and use data mining systems to analyze large amounts of data.

## Prerequisites
- Required prerequisite courses: CS 145 or CS 146 or equivalent. 
- You are expected to have background knowledge in data structures, algorithms, basic linear algebra, and basic statistics. 
- You are expected to know basic knowledge in data mining and machine learning. 
- You will also need to be familiar with at least one programming language (Python will be used for homework), and have programming experiences. 

## Learning Objectives
- Review and understand fundamentals of basic data mining techniques
- Learn recent data mining techniques on several advanced data types
- Develop skills to apply data mining algorithms to solve real-world applications
- Gain initial experience in conducting research on data mining

## Grading
- Homework: 50%
- Quizzes / Participation: 15%
- Course project: 35%

*All the deadlines are 11:59PM (midnight) of the due dates.

*Late submission policy: you will get original score * <a href="https://www.codecogs.com/eqnedit.php?latex=\mathbf{1}(t<=24)e^{-(ln(2)/12)*t}" target="_blank"><img src="https://latex.codecogs.com/gif.latex?\mathbf{1}(t<=24)e^{-(ln(2)/12)*t}" title="\mathbf{1}(t<=24)e^{-(ln(2)/12)*t}" /></a>, if you are t hours late.

*No copying or sharing of homework!


- You can discuss general challenges and ideas with others.
- Suspicious cases will be reported to The Office of the Dean of Students.

## Q & A
-	We will be using [Piazza](piazza.com/ucla/winter2022/cs247) for class discussion. The system is highly catered to getting you help fast and efficiently from classmates, the TAs, and myself. Rather than emailing questions to the teaching staff, I encourage you to post your questions on Piazza.
-	Sign up Piazza here: [piazza.com/ucla/winter2022/cs247](piazza.com/ucla/winter2022/cs247)
-	Tips: Answering other students' questions will increase your participation score.

## Academic Integrity Policy
"With its status as a world-class research institution, it is critical that the University uphold the highest standards of integrity both inside and outside the classroom. As a student and member of the UCLA community, you are expected to demonstrate integrity in all of your academic endeavors. Accordingly, when accusations of academic dishonesty occur, The Office of the Dean of Students is charged with investigating and adjudicating suspected violations. Academic dishonesty, includes, but is not limited to, cheating, fabrication, plagiarism, multiple submissions or facilitating academic misconduct."
For more information, please refer to the <a href="https://www.deanofstudents.ucla.edu/portals/16/documents/studentguide.pdf"> guidance </a>.

## Tentative Schedule
| Week | Date | Topic | Further Reading | Discussion Session| Homework| Course Project|
| ------- | ------ | ------ | -------- | ------ | ------ | ------ |
| Week 1 | 1/3 | Introduction [[Slides]](http://web.cs.ucla.edu/~yzsun/classes/2022Winter_CS247/Slides/01Intro.pdf) | <ul><li>[Review of probability from a course by David Blei](http://www.cs.princeton.edu/courses/archive/spring07/cos424/scribe_notes/0208.pdf) from Princeton U.</li><li>[Machine Learning Math Essentials](http://courses.washington.edu/css490/2012.Winter/lecture_slides/02_math_essentials.pdf) by Jeff Howbert from Washington U.</li><li>[http://cs229.stanford.edu/section/cs229-prob.pdf](http://cs229.stanford.edu/section/cs229-prob.pdf)</li><li>[Optimization](http://web.cs.ucla.edu/~yzsun/classes/2018Fall_CS145/Slides/optimization.pdf) | ------ | ------ | ------ |
| Week 1 | 1/5 | Basics: Logistic Regression and Naive Bayes [[Slides]](http://web.cs.ucla.edu/~yzsun/classes/2022Winter_CS247/Slides/02NaiveBayes_LR.pdf)| <ul><li>Naive Bayes: http://pages.cs.wisc.edu/~jerryzhu/cs769/nb.pdf </li><li>Newton Raphson Algorithm: https://www.stat.washington.edu/adobra/classes/536/Files/week1/newtonfull.pdf</li><li>Discriminative vs. generative: https://ai.stanford.edu/~ang/papers/nips01-discriminativegenerative.pdf</li></ul>| ------ | ------ | ------ |
| Week 2 | 1/10 |Basics: K-Means, Gaussian Mixture Model [[Slides]](http://web.cs.ucla.edu/~yzsun/classes/2022Winter_CS247/Slides/03kmeans_GMM.pdf)| <ul>Notes on mixture models and EM algorithm: <li>http://www.stat.cmu.edu/~cshalizi/350/lectures/29/lecture-29.pdf</li> <li>http://www.cs.ubc.ca/~murphyk/Teaching/CS340-Fall06/reading/mixtureModels.pdf</li> </ul> | ------ | ------ | ------ |
| Week 2 | 1/12 |Basics: Neural Networks, Deep Learning [[Slides]](http://web.cs.ucla.edu/~yzsun/classes/2022Winter_CS247/Slides/04NN_DeepLearning.pdf) | -------- | ------ | ------ | ------ |
| Week 3 | 1/17 | **Martin Luther King, Jr. holiday (No Class)** | -------- | ------ | ------ | ------ |
| Week 3 | 1/19 | Text: Topic Model: PLSA | -------- | ------ | ------ | ------ |
| Week 4 | 1/24 | Text: Topic Model: LDA [[Slides]](http://web.cs.ucla.edu/~yzsun/classes/2022Winter_CS247/Slides/05TopicModels.pdf)| -------- | ------ | ------ | ------ |
| Week 4 | 1/26 | Text: Word Embedding | -------- | ------ | ------ | ------ |
| Week 5 | 1/31 | Graph: Random Walk| -------- | ------ | ------ | ------ |
| Week 5 | 2/2 |  Graph: Label Propagation  | -------- | ------ | ------ | ------ |
| Week 6 | 2/7 | Graph: Spectral Clustering| -------- | ------ | ------ | ------ |
| Week 6 | 2/9 | Graph/Network Embedding | -------- | ------ | ------ | ------ |
| Week 7 | 2/14 | Graph: Knowledge Graph Embedding | -------- | ------ | ------ | ------ |
| Week 7 | 2/16 | RS: Collaborative Filtering, Matrix Factorization, and BPR | <ul><li>[Rendle et al., BPR: Bayesian Personalized Ranking from Implicit Feedback, UAI 2009](https://arxiv.org/ftp/arxiv/papers/1205/1205.2618.pdf)</li></ul> | ------ | ------ | ------ |
| Week 8 | 2/21 | **Presidentsâ€™ Day holiday (No Class)** | -------- | ------ | ------ | ------ |
| Week 8 | 2/23 | RS: Factorization Machine and Neural Collaborative Filtering | -------- | ------ | ------ | ------ |
| Week 9 | 2/28 | RS: Recommendation from Graph Perspective | -------- | ------ | ------ | ------ |
| Week 9 | 3/2 | TBD | -------- | ------ | ------ | ------ |
| Week 10 | 3/7 | Project Presentation | -------- | ------ | ------ | ------ |
| Week 10 | 3/9 | Project Presentation | -------- | ------ | ------ | Final Report / Code Due |
